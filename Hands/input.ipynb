{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras,lite\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw left hand connections\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    \n",
    "def draw_landmarks(image, results):\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.landmark[point].x, res.landmark[point].y, res.landmark[point].z] for point in mp_hands.HandLandmark for res in results.multi_hand_landmarks], dtype=float).flatten() if results.multi_hand_landmarks else np.zeros(21*3*2)\n",
    "    if lh.shape[0] == 63:\n",
    "        rh = np.array([[res.landmark[point].x, res.landmark[point].y, res.landmark[point].z] for point in mp_hands.HandLandmark for res in results.multi_hand_landmarks], dtype=float).flatten() if results.multi_hand_landmarks else np.zeros(21*3)\n",
    "        return np.concatenate([lh, rh])\n",
    "    # rh = np.array([[res.landmark[point].x, res.landmark[point].y, res.landmark[point].z] for point in mp_hands.HandLandmark for res in results.multi_hand_landmarks], dtype=float).flatten() if results.multi_hand_landmarks else np.zeros(21*3*2)\n",
    "    # return lh\n",
    "    return np.concatenate([lh])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'hii', 'by'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "# no_sequences = 10\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(0,no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Set mediapipe model \n",
    "    with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        \n",
    "        # NEW LOOP\n",
    "\n",
    "        # Loop through actions\n",
    "        for action in actions:\n",
    "            # Loop through sequences aka videos\n",
    "            for sequence in range(no_sequences):\n",
    "                # Loop through video length aka sequence length\n",
    "                for frame_num in range(sequence_length):\n",
    "\n",
    "                    # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    # Make detections\n",
    "                    image, results = mediapipe_detection(frame, hands)\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    draw_styled_landmarks(image, results)\n",
    "                    \n",
    "                    # NEW Apply wait logic\n",
    "                    if frame_num == 0: \n",
    "                        cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                        cv2.waitKey(500)\n",
    "                    else: \n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                    \n",
    "                    # NEW Export keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    # print(keypoints.shape)\n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                    np.save(npy_path, keypoints)\n",
    "\n",
    "                    # Break gracefully\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "finally:\n",
    "    # print(\"error\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 0, 'hii': 1, 'by': 2}\n"
     ]
    }
   ],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 126)\n",
      "(30,)\n",
      "(30, 30, 126)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "# print(sequences)\n",
    "print(np.array(sequences).shape)\n",
    "print(np.array(labels).shape)\n",
    "X = np.array(sequences)\n",
    "print(X.shape)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, input_shape=(30,126)))\n",
    "# model.add(LSTM(128, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=False))\n",
    "# model.add(Dense(64))\n",
    "# model.add(Dense(32))\n",
    "# model.add(Dense(actions.shape[0]))\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(sequence_length,X.shape[2])))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            48896     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203,459\n",
      "Trainable params: 203,459\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1044 - categorical_accuracy: 0.3214\n",
      "Epoch 2/60\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 1.0922 - categorical_accuracy: 0.3929\n",
      "Epoch 3/60\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 1.0834 - categorical_accuracy: 0.7143\n",
      "Epoch 4/60\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 1.0703 - categorical_accuracy: 0.6071\n",
      "Epoch 5/60\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 1.0564 - categorical_accuracy: 0.5714\n",
      "Epoch 6/60\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.0305 - categorical_accuracy: 0.7143\n",
      "Epoch 7/60\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.9927 - categorical_accuracy: 0.8571\n",
      "Epoch 8/60\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.9246 - categorical_accuracy: 0.8214\n",
      "Epoch 9/60\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.8098 - categorical_accuracy: 0.8214\n",
      "Epoch 10/60\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.6258 - categorical_accuracy: 0.7857\n",
      "Epoch 11/60\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.5104 - categorical_accuracy: 0.8214\n",
      "Epoch 12/60\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 3.8444 - categorical_accuracy: 0.5714\n",
      "Epoch 13/60\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.4070 - categorical_accuracy: 0.9286\n",
      "Epoch 14/60\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.5928 - categorical_accuracy: 0.7857\n",
      "Epoch 15/60\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.7181 - categorical_accuracy: 0.6429\n",
      "Epoch 16/60\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.7434 - categorical_accuracy: 0.6429\n",
      "Epoch 17/60\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.7411 - categorical_accuracy: 0.6429\n",
      "Epoch 18/60\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.7331 - categorical_accuracy: 0.6786\n",
      "Epoch 19/60\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.7203 - categorical_accuracy: 0.7500\n",
      "Epoch 20/60\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.7036 - categorical_accuracy: 0.7857\n",
      "Epoch 21/60\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.6826 - categorical_accuracy: 0.7500\n",
      "Epoch 22/60\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.6573 - categorical_accuracy: 0.8214\n",
      "Epoch 23/60\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.6195 - categorical_accuracy: 0.8929\n",
      "Epoch 24/60\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.5722 - categorical_accuracy: 0.8571\n",
      "Epoch 25/60\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.5136 - categorical_accuracy: 0.8571\n",
      "Epoch 26/60\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.4560 - categorical_accuracy: 0.8929\n",
      "Epoch 27/60\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.4260 - categorical_accuracy: 0.8929\n",
      "Epoch 28/60\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.4072 - categorical_accuracy: 0.7857\n",
      "Epoch 29/60\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.5099 - categorical_accuracy: 0.7500\n",
      "Epoch 30/60\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.6003 - categorical_accuracy: 0.7857\n",
      "Epoch 31/60\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.5199 - categorical_accuracy: 0.7857\n",
      "Epoch 32/60\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 3.3564 - categorical_accuracy: 0.7500\n",
      "Epoch 33/60\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.5546 - categorical_accuracy: 0.8571\n",
      "Epoch 34/60\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 18.7335 - categorical_accuracy: 0.6429\n",
      "Epoch 35/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 4.1768 - categorical_accuracy: 0.7857\n",
      "Epoch 36/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 2.0300 - categorical_accuracy: 0.8571\n",
      "Epoch 37/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.9789 - categorical_accuracy: 0.8214\n",
      "Epoch 38/60\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.6795 - categorical_accuracy: 0.7143\n",
      "Epoch 39/60\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.6928 - categorical_accuracy: 0.7143\n",
      "Epoch 40/60\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.7337 - categorical_accuracy: 0.7143\n",
      "Epoch 41/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.7631 - categorical_accuracy: 0.6429\n",
      "Epoch 42/60\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.7805 - categorical_accuracy: 0.6429\n",
      "Epoch 43/60\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.7722 - categorical_accuracy: 0.6786\n",
      "Epoch 44/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.7286 - categorical_accuracy: 0.6786\n",
      "Epoch 45/60\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.6833 - categorical_accuracy: 0.7143\n",
      "Epoch 46/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.6676 - categorical_accuracy: 0.7500\n",
      "Epoch 47/60\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.6460 - categorical_accuracy: 0.7500\n",
      "Epoch 48/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.5969 - categorical_accuracy: 0.7857\n",
      "Epoch 49/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.5516 - categorical_accuracy: 0.8571\n",
      "Epoch 50/60\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.5502 - categorical_accuracy: 0.8214\n",
      "Epoch 51/60\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.5024 - categorical_accuracy: 0.8571\n",
      "Epoch 52/60\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.4623 - categorical_accuracy: 0.8214\n",
      "Epoch 53/60\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.4683 - categorical_accuracy: 0.8571\n",
      "Epoch 54/60\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.3906 - categorical_accuracy: 0.8571\n",
      "Epoch 55/60\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.4131 - categorical_accuracy: 0.8214\n",
      "Epoch 56/60\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.3679 - categorical_accuracy: 0.8571\n",
      "Epoch 57/60\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.3940 - categorical_accuracy: 0.8571\n",
      "Epoch 58/60\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.2738 - categorical_accuracy: 0.9643\n",
      "Epoch 59/60\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.3355 - categorical_accuracy: 0.8571\n",
      "Epoch 60/60\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2745 - categorical_accuracy: 0.8929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c201aebfa0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=60, callbacks=[tb_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 356ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.8107585e-10, 9.9995816e-01, 4.1829247e-05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(X_test)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "position = 1\n",
    "print(actions[np.argmax(res[position])])\n",
    "print(actions[np.argmax(y_test[position])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"action.h5\")\n",
    "# tf.keras.models.save_model(model,\"saved_model.pb\")\n",
    "tf.keras.models.save_model(model,\"action.h5\")\n",
    "# converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "# tfmodel = converter.convert()\n",
    "\n",
    "# open(os.path.join(DATA_PATH, \"action.tflite\"),\"wb\").write(tfmodel)\n",
    "# del model\n",
    "model.load_weights(\"action.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: save_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: save_model/assets\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "<unknown>:0: error: loc(callsite(callsite(fused[\"TensorListReserve:\", \"sequential_16/lstm_35/TensorArrayV2_1@__inference__wrapped_model_347747\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_350140\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: error: loc(callsite(callsite(fused[\"TensorListReserve:\", \"sequential_16/lstm_35/TensorArrayV2_1@__inference__wrapped_model_347747\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_350140\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\D2D\\0 SSIP Hackathon\\Sign_Language_To_Text_Converter_Hackathon\\input.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/D2D/0%20SSIP%20Hackathon/Sign_Language_To_Text_Converter_Hackathon/input.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39msave(filepath\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msave_model/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/D2D/0%20SSIP%20Hackathon/Sign_Language_To_Text_Converter_Hackathon/input.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m converter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mTFLiteConverter\u001b[39m.\u001b[39mfrom_saved_model(\u001b[39m'\u001b[39m\u001b[39msave_model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/D2D/0%20SSIP%20Hackathon/Sign_Language_To_Text_Converter_Hackathon/input.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tflite_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/D2D/0%20SSIP%20Hackathon/Sign_Language_To_Text_Converter_Hackathon/input.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maction.tflite\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mwrite(tflite_model)\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:930\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[0;32m    928\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    929\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 930\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:908\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_conversion_params_metric()\n\u001b[0;32m    907\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[1;32m--> 908\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    909\u001b[0m elapsed_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m    910\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1213\u001b[0m, in \u001b[0;36mTFLiteSavedModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1209\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_debug_info \u001b[39m=\u001b[39m _get_debug_info(\n\u001b[0;32m   1210\u001b[0m       _convert_debug_info_func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trackable_obj\u001b[39m.\u001b[39mgraph_debug_info),\n\u001b[0;32m   1211\u001b[0m       graph_def)\n\u001b[1;32m-> 1213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_from_saved_model(graph_def)\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1096\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2._convert_from_saved_model\u001b[1;34m(self, graph_def)\u001b[0m\n\u001b[0;32m   1093\u001b[0m converter_kwargs\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_base_converter_args())\n\u001b[0;32m   1094\u001b[0m converter_kwargs\u001b[39m.\u001b[39mupdate(quant_mode\u001b[39m.\u001b[39mconverter_flags())\n\u001b[1;32m-> 1096\u001b[0m result \u001b[39m=\u001b[39m _convert_saved_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconverter_kwargs)\n\u001b[0;32m   1097\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimize_tflite_model(\n\u001b[0;32m   1098\u001b[0m     result, quant_mode, quant_io\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_new_quantizer)\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:212\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     report_error_message(\u001b[39mstr\u001b[39m(converter_error))\n\u001b[1;32m--> 212\u001b[0m   \u001b[39mraise\u001b[39;00m converter_error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m  \u001b[39m# Re-throws the exception.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    214\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    204\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[0;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:809\u001b[0m, in \u001b[0;36mconvert_saved_model\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m model_flags \u001b[39m=\u001b[39m build_model_flags(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    808\u001b[0m conversion_flags \u001b[39m=\u001b[39m build_conversion_flags(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 809\u001b[0m data \u001b[39m=\u001b[39m convert(\n\u001b[0;32m    810\u001b[0m     model_flags\u001b[39m.\u001b[39;49mSerializeToString(),\n\u001b[0;32m    811\u001b[0m     conversion_flags\u001b[39m.\u001b[39;49mSerializeToString(),\n\u001b[0;32m    812\u001b[0m     input_data_str\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    813\u001b[0m     debug_info_str\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    814\u001b[0m     enable_mlir_converter\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:311\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[39mfor\u001b[39;00m error_data \u001b[39min\u001b[39;00m _metrics_wrapper\u001b[39m.\u001b[39mretrieve_collected_errors():\n\u001b[0;32m    310\u001b[0m       converter_error\u001b[39m.\u001b[39mappend_error(error_data)\n\u001b[1;32m--> 311\u001b[0m     \u001b[39mraise\u001b[39;00m converter_error\n\u001b[0;32m    313\u001b[0m \u001b[39mreturn\u001b[39;00m _run_deprecated_conversion_binary(model_flags_str,\n\u001b[0;32m    314\u001b[0m                                          conversion_flags_str, input_data_str,\n\u001b[0;32m    315\u001b[0m                                          debug_info_str)\n",
      "\u001b[1;31mConverterError\u001b[0m: <unknown>:0: error: loc(callsite(callsite(fused[\"TensorListReserve:\", \"sequential_16/lstm_35/TensorArrayV2_1@__inference__wrapped_model_347747\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_350140\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: error: loc(callsite(callsite(fused[\"TensorListReserve:\", \"sequential_16/lstm_35/TensorArrayV2_1@__inference__wrapped_model_347747\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_350140\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\n"
     ]
    }
   ],
   "source": [
    "model.save(filepath=\"save_model/\")\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('save_model')\n",
    "tflite_model = converter.convert()\n",
    "open(\"action.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "tfmodel = converter.convert()\n",
    "\n",
    "open(os.path.join(DATA_PATH, \"action.tflite\"),\"wb\").write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "NewRandomAccessFile failed to Create/Open: saved_model.pb : Access is denied.\r\n; Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\D2D\\0 SSIP Hackathon\\Sign_Language_To_Text_Converter_Hackathon\\input.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/D2D/0%20SSIP%20Hackathon/Sign_Language_To_Text_Converter_Hackathon/input.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39maction.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/D2D/0%20SSIP%20Hackathon/Sign_Language_To_Text_Converter_Hackathon/input.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m converter \u001b[39m=\u001b[39m lite\u001b[39m.\u001b[39;49mTFLiteConverter\u001b[39m.\u001b[39;49mfrom_saved_model(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/D2D/0%20SSIP%20Hackathon/Sign_Language_To_Text_Converter_Hackathon/input.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# converter = lite.TFLiteConverter.from_keras_model(model)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/D2D/0%20SSIP%20Hackathon/Sign_Language_To_Text_Converter_Hackathon/input.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tflite_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39mconvert()\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1789\u001b[0m, in \u001b[0;36mTFLiteConverterV2.from_saved_model\u001b[1;34m(cls, saved_model_dir, signature_keys, tags)\u001b[0m\n\u001b[0;32m   1786\u001b[0m   tags \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([_tag_constants\u001b[39m.\u001b[39mSERVING])\n\u001b[0;32m   1788\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39meager_mode():\n\u001b[1;32m-> 1789\u001b[0m   saved_model \u001b[39m=\u001b[39m _load(saved_model_dir, tags)\n\u001b[0;32m   1790\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signature_keys:\n\u001b[0;32m   1791\u001b[0m   signature_keys \u001b[39m=\u001b[39m saved_model\u001b[39m.\u001b[39msignatures\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:800\u001b[0m, in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(export_dir, os\u001b[39m.\u001b[39mPathLike):\n\u001b[0;32m    799\u001b[0m   export_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(export_dir)\n\u001b[1;32m--> 800\u001b[0m result \u001b[39m=\u001b[39m load_partial(export_dir, \u001b[39mNone\u001b[39;49;00m, tags, options)[\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    801\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:905\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[39mif\u001b[39;00m tags \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tags, \u001b[39mset\u001b[39m):\n\u001b[0;32m    901\u001b[0m   \u001b[39m# Supports e.g. tags=SERVING and tags=[SERVING]. Sets aren't considered\u001b[39;00m\n\u001b[0;32m    902\u001b[0m   \u001b[39m# sequences for nest.flatten, so we put those through as-is.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m   tags \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mflatten(tags)\n\u001b[0;32m    904\u001b[0m saved_model_proto, debug_info \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 905\u001b[0m     loader_impl\u001b[39m.\u001b[39;49mparse_saved_model_with_debug_info(export_dir))\n\u001b[0;32m    907\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(saved_model_proto\u001b[39m.\u001b[39mmeta_graphs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     saved_model_proto\u001b[39m.\u001b[39mmeta_graphs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mHasField(\u001b[39m\"\u001b[39m\u001b[39mobject_graph_def\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    909\u001b[0m   metrics\u001b[39m.\u001b[39mIncrementReadApi(_LOAD_V2_LABEL)\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:57\u001b[0m, in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_saved_model_with_debug_info\u001b[39m(export_dir):\n\u001b[0;32m     45\u001b[0m   \u001b[39m\"\"\"Reads the savedmodel as well as the graph debug info.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m    parsed. Missing graph debug info file is fine.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m   saved_model \u001b[39m=\u001b[39m parse_saved_model(export_dir)\n\u001b[0;32m     59\u001b[0m   debug_info_path \u001b[39m=\u001b[39m file_io\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     60\u001b[0m       saved_model_utils\u001b[39m.\u001b[39mget_debug_dir(export_dir),\n\u001b[0;32m     61\u001b[0m       constants\u001b[39m.\u001b[39mDEBUG_INFO_FILENAME_PB)\n\u001b[0;32m     62\u001b[0m   debug_info \u001b[39m=\u001b[39m graph_debug_info_pb2\u001b[39m.\u001b[39mGraphDebugInfo()\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:100\u001b[0m, in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m file_io\u001b[39m.\u001b[39mfile_exists(path_to_pb):\n\u001b[0;32m     99\u001b[0m   \u001b[39mwith\u001b[39;00m file_io\u001b[39m.\u001b[39mFileIO(path_to_pb, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 100\u001b[0m     file_content \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m    101\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     saved_model\u001b[39m.\u001b[39mParseFromString(file_content)\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:114\u001b[0m, in \u001b[0;36mFileIO.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    103\u001b[0m   \u001b[39m\"\"\"Returns the contents of a file as a string.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[39m  Starts reading from current position in file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m    string if in string (regular) mode.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preread_check()\n\u001b[0;32m    115\u001b[0m   \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    116\u001b[0m     length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Installed_software\\python\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:76\u001b[0m, in \u001b[0;36mFileIO._preread_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_check_passed:\n\u001b[0;32m     74\u001b[0m   \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mPermissionDeniedError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m                                      \u001b[39m\"\u001b[39m\u001b[39mFile isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt open for reading\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_buf \u001b[39m=\u001b[39m _pywrap_file_io\u001b[39m.\u001b[39;49mBufferedInputStream(\n\u001b[0;32m     77\u001b[0m     compat\u001b[39m.\u001b[39;49mpath_to_str(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name), \u001b[39m1024\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m512\u001b[39;49m)\n",
      "\u001b[1;31mUnknownError\u001b[0m: NewRandomAccessFile failed to Create/Open: saved_model.pb : Access is denied.\r\n; Input/output error"
     ]
    }
   ],
   "source": [
    "model = load_model('action.h5')\n",
    "converter = lite.TFLiteConverter.from_saved_model(\"\")\n",
    "# converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"action_new.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[1, 0],\n",
       "        [0, 1]]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "\n",
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    # Read an image, flip it around y-axis for correct handedness output (see\n",
    "    # above).\n",
    "    image = cv2.flip(cv2.imread(file), 1)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print handedness and draw hand landmarks on the image.\n",
    "    print('Handedness:', results.multi_handedness)\n",
    "    if not results.multi_hand_landmarks:\n",
    "      continue\n",
    "    image_height, image_width, _ = image.shape\n",
    "    annotated_image = image.copy()\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "      print('hand_landmarks:', hand_landmarks)\n",
    "      print(\n",
    "          f'Index finger tip coordinates: (',\n",
    "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "      )\n",
    "      mp_drawing.draw_landmarks(\n",
    "          annotated_image,\n",
    "          hand_landmarks,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())\n",
    "    cv2.imwrite(\n",
    "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
    "    # Draw hand world landmarks.\n",
    "    if not results.multi_hand_world_landmarks:\n",
    "      continue\n",
    "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
    "      mp_drawing.plot_landmarks(\n",
    "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "      break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ce6ccc11a83bc9bed0c17f1af93ce616e6495f116eabf1a0084cc277f1779e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
